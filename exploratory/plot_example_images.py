"""
Generate comparison images using various text-to-image diffusion models.

This script produces a visual comparison of images generated by either
Stable Diffusion (SD), Fine-tuned Diffusion Model (FDM), Debias Diffusion (DD),
Attribute Switching (AS), or Fair Diffusion (FD).

Usage:
    python exploratory/generate_comparison_images.py --models SD FDM DD AS FD
                                         --prompts "a photo of a doctor" "a portrait of a CEO"
                                         --num_images_per_prompt 4

Outputs:
    1. Individual Images:
       - Location: PROJECT_ROOT/results/section_5.4/comparison_images/<model_name>/<prompt_slug>/
       - Format: PNG files named "<prompt_slug>_<seed>_<image_number>.png"

    2. Image Grids:
       - Location: PROJECT_ROOT/results/section_5.4/comparison_images/<model_name>/
       - Format: PNG files named "<prompt_slug>_<seed>_grid.png"
       - Content: Grid of num_images_per_prompt images for each prompt and model

Arguments:
    --models: List of models to use (choices: SD, FDM, DD, AS, FD)
    --model_id: HuggingFace model ID or local path (default: PalionTech/debias-diffusion-orig)
    --prompts: List of text prompts for image generation
    --output_dir: Custom output directory path
    --num_images_per_prompt: Number of images per prompt (default: 4)
    --num_inference_steps: Denoising steps for generation (default: 50)
    --guidance_scale: Classifier-free guidance scale (default: 7.5)
    --grid_cols: Number of columns in output grids (default: 2)
    --seed: Random seed for reproducibility (default: 42)
    --device: Computation device (default: cuda if available, else cpu)
"""

import argparse
import random
import sys
from pathlib import Path
from typing import List, Dict, Union

import torch
from diffusers import StableDiffusionPipeline
from diffusers.loaders import LoraLoaderMixin
from PIL import Image
from tqdm import tqdm

# Add the project root to the Python path
SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent
sys.path.append(str(PROJECT_ROOT))

from src.pipelines.attribute_switching_pipeline import AttributeSwitchingPipeline
from src.pipelines.debias_diffusion_pipeline import DebiasDiffusionPipeline
from src.pipelines.fair_diffusion_pipeline import SemanticStableDiffusionPipeline
from src.utils.image_utils import create_image_grid, save_images

def set_seed(seed: int) -> None:
    """Set random seed for reproducibility."""
    random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def setup_pipeline(model: str, model_id: str, device: str) -> Union[
    StableDiffusionPipeline,
    DebiasDiffusionPipeline,
    AttributeSwitchingPipeline,
    SemanticStableDiffusionPipeline
]:
    """Set up the appropriate pipeline based on the model type."""
    if model == "SD":
        return StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(device)
    elif model == "FDM":
        pipeline = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(device)
        text_encoder_lora_params = LoraLoaderMixin._modify_text_encoder(pipeline.text_encoder, dtype=torch.float32, rank=50, patch_mlp=False)
        text_encoder_lora_dict = torch.load(PROJECT_ROOT / "data/model_data/FDM_weights/text_encoder_lora_EMA_rag.pth", map_location=device)
        _ = pipeline.text_encoder.load_state_dict(text_encoder_lora_dict, strict=False)
        return pipeline
    elif model == "DD":
        pipeline = DebiasDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(device)
        classifiers_base_path = PROJECT_ROOT / "data/model_data/h_space_classifiers/version_2/5k"
        for attr, params in {
            "gender": ([0.5, 0.5], (0, 0.5), 2),
            "race": ([0.25, 0.25, 0.25, 0.25], (0, 0.75), 4),
            "age": ([0.75, 0.25], (0, 1.125), 2)
        }.items():
            pipeline.set_attribute_params(
                attribute=attr,
                distribution=params[0],
                bias_range=params[1],
                classifier_path=classifiers_base_path / f"{attr}_5k_e100_bs256_lr0.0001_tv0.8/best_model.pt",
                num_classes=params[2],
                model_type="linear",
                default_assignments=None,
                default_switch_step=None,
            )
        pipeline.set_tau_bias(19)
        pipeline.set_iota_step_range([4, 19])
        pipeline.set_debiasing_options(use_debiasing=True, use_distribution_guidance=True, interpolation_method='linear')
        return pipeline
    elif model == "AS":
        pipeline = AttributeSwitchingPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(device)
        attribute_switch_steps = {"gender": 18, "race": 19, "age": 18}
        attribute_weights = {"gender": [1,1], "race": [1,1,1,1], "age": [3,1]}
        for attr, step in attribute_switch_steps.items():
            pipeline.set_attribute_params(attr, step, attribute_weights[attr])
        pipeline.set_debiasing_options(True)
        return pipeline
    elif model == "FD":
        pipeline = SemanticStableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(device)
        pipeline.set_momentum(scale=0.3, beta=0.6)
        editing_prompts = {
            "gender": ["male person", "female person"],
            "age": ["young person", "old person"],
            "race": ["white person", "black person", "asian person", "indian person"]
        }
        edit_params = {
            "edit_warmup_steps": {"gender": [10, 10], "age": [5, 5], "race": [5, 5, 5, 5]},
            "edit_guidance_scales": {"gender": [6, 6], "age": [3, 3], "race": [4, 4, 4, 4]},
            "edit_thresholds": {"gender": [0.95, 0.95], "age": [0.95, 0.95], "race": [0.95, 0.95, 0.95, 0.95]},
            "edit_weights": {"gender": [1, 1], "age": [3, 1], "race": [1, 1, 1, 1]}
        }
        pipeline.set_attribute_params(editing_prompts, **edit_params)
        return pipeline
    else:
        raise ValueError(f"Unknown model type: {model}")

def generate_images(pipeline: Union[StableDiffusionPipeline, DebiasDiffusionPipeline, AttributeSwitchingPipeline, SemanticStableDiffusionPipeline],
                    prompts: List[str],
                    num_images_per_prompt: int,
                    num_inference_steps: int,
                    guidance_scale: float,
                    seed: int) -> List[Image.Image]:
    """Generate images using the provided pipeline and prompts."""
    all_images = []
    for prompt in tqdm(prompts, desc="Generating images"):
        if isinstance(pipeline, SemanticStableDiffusionPipeline):
            attributes = pipeline.editing_prompts.keys()
            choices = [0 for _ in attributes]
            reverse_editing_direction = [i == choice for attr in attributes for i, choice in 
                    zip(range(2 if attr != 'race' else 4), [choices.pop(0)] * (2 if attr != 'race' else 4))]
            images = pipeline(
                [prompt] * num_images_per_prompt,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                generator=torch.Generator(device=pipeline.device).manual_seed(seed),
                reverse_editing_direction=reverse_editing_direction
            ).images
        else:
            images = pipeline(
                [prompt] * num_images_per_prompt,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                generator=torch.Generator(device=pipeline.device).manual_seed(seed)
            ).images
        all_images.extend(images)
    return all_images

def main(args: argparse.Namespace) -> None:
    set_seed(args.seed)
    
    for model in args.models:
        print(f"Setting up {model} pipeline...")
        pipeline = setup_pipeline(model, args.model_id, args.device)
        
        print(f"Generating images for {model}...")
        images = generate_images(pipeline, args.prompts, args.num_images_per_prompt,
                                 args.num_inference_steps, args.guidance_scale, args.seed)
        
        print(f"Saving images for {model}...")
        output_dir = Path(args.output_dir) / model
        save_images(images, output_dir, args.prompts, args.num_images_per_prompt,
                    args.grid_cols, args.seed)
        
        print(f"Images for {model} saved to {output_dir}")
        
        # Clear CUDA cache to free up memory
        if args.device == "cuda":
            torch.cuda.empty_cache()

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Generate comparison images using various text-to-image diffusion models")
    parser.add_argument("--models", nargs="+", choices=["SD", "FDM", "DD", "AS", "FD"], required=True,
                        help="Types of models to use")
    parser.add_argument("--model_id", type=str, default="PalionTech/debias-diffusion-orig",
                        help="Hugging Face model ID or path to local model")
    parser.add_argument("--prompts", nargs="+", default=["a photo of the face of a senator"],
                        help="List of prompts to generate images from")
    parser.add_argument("--output_dir", type=str, default=PROJECT_ROOT / "results/example_images",
                        help="Output directory for generated images")
    parser.add_argument("--num_images_per_prompt", type=int, default=4,
                        help="Number of images to generate per prompt")
    parser.add_argument("--num_inference_steps", type=int, default=50,
                        help="Number of denoising steps")
    parser.add_argument("--guidance_scale", type=float, default=7.5,
                        help="Guidance scale for classifier-free guidance")
    parser.add_argument("--grid_cols", type=int, default=6,
                        help="Number of columns in the output grid")
    parser.add_argument("--seed", type=int, default=42,
                        help="Random seed for reproducibility")
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu",
                        help="Device to run the model on")
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    main(args)